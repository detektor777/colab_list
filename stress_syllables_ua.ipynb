{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9rWA47NLu7DvjXNKMyAfE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/detektor777/colab_list/blob/main/stress_syllables_ua.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-e1hzNFeJ1TK"
      },
      "outputs": [],
      "source": [
        "#@title ##**Run** { display-mode: \"form\" }\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "import unicodedata\n",
        "import regex  # Use regex module\n",
        "\n",
        "custom_dict = {}\n",
        "\n",
        "def normalize_text(text):\n",
        "    return unicodedata.normalize('NFC', text)\n",
        "\n",
        "def remove_combining_chars(text):\n",
        "    decomposed = unicodedata.normalize('NFD', text)\n",
        "    filtered = ''.join(c for c in decomposed if unicodedata.category(c) != 'Mn')\n",
        "    return unicodedata.normalize('NFC', filtered)\n",
        "\n",
        "def convert_accented_text(text):\n",
        "    result = \"\"\n",
        "    for char in text:\n",
        "        decomposed = unicodedata.normalize('NFD', char)\n",
        "        if any('COMBINING ACUTE ACCENT' in unicodedata.name(c, '') for c in decomposed):\n",
        "            base_char = ''.join([c for c in decomposed if 'COMBINING ACUTE ACCENT' not in unicodedata.name(c, '')])\n",
        "            result += unicodedata.normalize('NFC', base_char) + \"+\"\n",
        "        else:\n",
        "            result += unicodedata.normalize('NFC', char)\n",
        "    return result\n",
        "\n",
        "def adjust_case(original, replacement):\n",
        "    if original.isupper():\n",
        "        return replacement.upper()\n",
        "    elif original[0].isupper() and original[1:].islower():\n",
        "        return replacement.capitalize()\n",
        "    elif original.islower():\n",
        "        return replacement.lower()\n",
        "    else:\n",
        "        adjusted = ''\n",
        "        for o_char, r_char in zip(original, replacement):\n",
        "            if o_char.isupper():\n",
        "                adjusted += r_char.upper()\n",
        "            else:\n",
        "                adjusted += r_char.lower()\n",
        "        adjusted += replacement[len(original):]\n",
        "        return adjusted\n",
        "\n",
        "def replace_with_custom_dict(text):\n",
        "    text = normalize_text(text)\n",
        "    tokens = regex.findall(r'[\\p{L}\\p{M}\\+]+|\\s+|[^\\s\\p{L}\\p{M}]+', text)\n",
        "    new_tokens = []\n",
        "    for token in tokens:\n",
        "        token_normalized = normalize_text(token)\n",
        "        if regex.match(r'^[\\p{L}\\p{M}\\+]+$', token_normalized):\n",
        "            token_no_combining = remove_combining_chars(token_normalized)\n",
        "            base_token = token_no_combining.replace('+', '').lower()\n",
        "            base_token = normalize_text(base_token)\n",
        "            if base_token in custom_dict:\n",
        "                replacement = custom_dict[base_token]\n",
        "                adjusted_replacement = adjust_case(token, replacement)\n",
        "                new_tokens.append(adjusted_replacement)\n",
        "            else:\n",
        "                new_tokens.append(token)\n",
        "        else:\n",
        "            new_tokens.append(token)\n",
        "    return ''.join(new_tokens)\n",
        "\n",
        "def send_post_request(text):\n",
        "    url = \"https://slovnyk.ua/nagolos.php\"\n",
        "    data = {'text': text}\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "    try:\n",
        "        response = requests.post(url, data=data, headers=headers)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            emph_content = soup.find(id=\"emph\")\n",
        "            if emph_content:\n",
        "                for br in emph_content.find_all(\"br\"):\n",
        "                    br.replace_with(\"\\n\")\n",
        "                plain_text = emph_content.get_text()\n",
        "                processed_text = replace_with_custom_dict(plain_text.strip())\n",
        "                processed_text = convert_accented_text(processed_text)\n",
        "                output.value = processed_text\n",
        "            else:\n",
        "                output.value = \"Element with id='emph' not found.\"\n",
        "        else:\n",
        "            output.value = f\"Error: {response.status_code}\"\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        output.value = f\"Connection error: {e}\"\n",
        "\n",
        "def on_submit_button_clicked(b):\n",
        "    text = text_input.value\n",
        "    if text:\n",
        "        send_post_request(text)\n",
        "    else:\n",
        "        output.value = \"Please enter text.\"\n",
        "\n",
        "def on_file_upload_change(change):\n",
        "    global custom_dict\n",
        "    if file_upload.value:\n",
        "        uploaded_file = next(iter(file_upload.value.values()))\n",
        "        content = uploaded_file['content'].decode('utf-8')\n",
        "        lines = content.strip().split('\\n')\n",
        "        custom_dict = {}\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                line_normalized = normalize_text(line)\n",
        "                base_word = remove_combining_chars(line_normalized.replace('+', '').lower())\n",
        "                custom_dict[base_word] = line_normalized\n",
        "\n",
        "try:\n",
        "    import regex\n",
        "except ImportError:\n",
        "    !pip install regex\n",
        "    import regex\n",
        "\n",
        "file_upload = widgets.FileUpload(\n",
        "    accept='.txt',\n",
        "    multiple=False\n",
        ")\n",
        "file_upload.observe(on_file_upload_change, names='value')\n",
        "\n",
        "text_input = widgets.Textarea(\n",
        "    description='Text:',\n",
        "    placeholder='Enter text...',\n",
        "    layout=widgets.Layout(width='90%', height='200px')\n",
        ")\n",
        "submit_button = widgets.Button(description=\"Submit\")\n",
        "\n",
        "output = widgets.Textarea(\n",
        "    description='Result:',\n",
        "    disabled=True,\n",
        "    layout=widgets.Layout(width='90%', height='200px')\n",
        ")\n",
        "\n",
        "submit_button.on_click(on_submit_button_clicked)\n",
        "\n",
        "display(widgets.HBox([file_upload, widgets.Label('Custom Dictionary')]))\n",
        "display(text_input, submit_button, output)\n"
      ]
    }
  ]
}